{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGradient(object):\n",
    "    def hypothesis(self, x, a): \n",
    "        return np.dot(X, a)\n",
    "\n",
    "    def gradient(self, x, y, a):\n",
    "        h = self.hypothesis(x, a) \n",
    "        grad = np.dot(X.transpose(), (h - y)) \n",
    "        return grad\n",
    "\n",
    "    def gradient_descend(self, x, y):\n",
    "        theta = np.zeros((x.shape[1], 1)) \n",
    "        error_list = [] \n",
    "        max_iters = 3\n",
    "        for itr in range(max_iters): \n",
    "            theta, errors = self.gradient_descend_step(x, y)\n",
    "            error_list.extend(errors)\n",
    "\n",
    "        return theta, error_list\n",
    "\n",
    "    def gradient_descend_step(self, x, y):\n",
    "        return 0, []\n",
    "\n",
    "\n",
    "def do_gradiend_descend(gd_alg):\n",
    "    mean = np.array([5.0, 6.0]) \n",
    "    cov = np.array([[1.0, 0.95], [0.95, 1.2]]) \n",
    "    data = np.random.multivariate_normal(mean, cov, 8000) \n",
    "    \n",
    "    # visualising data \n",
    "    plt.scatter(data[:500, 0], data[:500, 1], marker = '.') \n",
    "    plt.show() \n",
    "    \n",
    "    # train-test-split \n",
    "    data = np.hstack((np.ones((data.shape[0], 1)), data)) \n",
    "    \n",
    "    split_factor = 0.90\n",
    "    split = int(split_factor * data.shape[0]) \n",
    "    \n",
    "    X_train = data[:split, :-1] \n",
    "    y_train = data[:split, -1].reshape((-1, 1)) \n",
    "    X_test = data[split:, :-1] \n",
    "    y_test = data[split:, -1].reshape((-1, 1)) \n",
    "\n",
    "    theta, errors = gd_alg.gradient_descend(X_train, y_train)\n",
    "    plt.plot(error_list) \n",
    "    plt.xlabel(\"Number of iterations\") \n",
    "    plt.ylabel(\"Cost\") \n",
    "    plt.show()\n",
    "\n",
    "    y_pred = gd_alg.hypothesis(X_test, theta) \n",
    "    plt.scatter(X_test[:, 1], y_test[:, ], marker = '.') \n",
    "    plt.plot(X_test[:, 1], y_pred, color = 'orange') \n",
    "    plt.show() \n",
    "    \n",
    "    # calculating error in predictions \n",
    "    error = np.sum(np.abs(y_test - y_pred) / y_test.shape[0]) \n",
    "    print(\"Mean absolute error = \", error)"
   ]
  }
 ]
}